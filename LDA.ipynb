{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pyspark import SparkConf, SparkContext, SQLContext \n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import split, explode, udf, lit, size, col\n",
    "from pyspark.ml.feature import RegexTokenizer, CountVectorizer\n",
    "from pyspark.ml.clustering import LDA\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"DataFrame\").getOrCreate()\n",
    "sc = SparkContext.getOrCreate()\n",
    "rdd = sc.wholeTextFiles(\"gs://mmj2169hw2/hadoop/tmp/bigquery/pyspark_output/lda_file/lda.txt/*\")\n",
    "\n",
    "tweets = rdd.map(lambda x: x[1].split('\\n'))\n",
    "tweet_array = tweets.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- tweets: string (nullable = true)\n",
      "\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|tweets                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|à¸£à¸±à¸šà¸‡à¸²à¸™ à¸¡à¸²à¹ƒà¸«à¸¡à¹ˆà¸„à¹ˆà¸° à¸§à¹ˆà¸§à¸‡à¸„à¹ˆà¸°                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |\n",
      "|# à¸£à¸±â€¦ https://t.co/4ujyL8e2vXRT @luvswbn: LATAM needs 2gether the movie                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |\n",
      "|#à¸„à¸±à¹ˆà¸™à¸à¸¹themovieLATAM #winmetawin @winmetawin https://t.co/rkwASdDmDb@scodarko to brincando Jupy mais e ai vc ta bem depois de tudo isso?Ø¹Ø§ØµÙ…Û Ø´ÛŒØ±Ø§Ø²ÛŒ Ø¬ÛŒØ³ÛŒ Ø­Ø±Ø§Ù… Ø®ÙˆØ± Ø¹ÙˆØ±ØªÙˆÚº Ø§ÙˆØ± Ù…Ø±Ø¯ÙˆÚº Ù…ÛŒÚº Ø­Ø±Ø§Ù… Ú©Û’ Ù¾ÛŒØ³Û’  Ø§Ù†Ú©ÛŒ Ø±ÙˆØ­ Ú©ÛŒ ØºØ°Ø§ Ø¨Ù† Ú†Ú©Û’ ÛÛŒÚºÛ”ÛŒÛ Ø¬Ø¨ ØªÚ© Ø­Ø±Ø§Ù… Ú©Ø§ Ù„Ù‚Ù…Û Ú©Ú¾Ø§â€¦ https://t.co/rjOi9DUD6lRT @yenaviral: \"The 71st episode of ë½•ìˆ­ì•„í•™ë‹¹, aired on the 20th, soared to 5.9% nationwide and 7.1% per minute according to Nielsen Korea, ranâ€¦RT @Hell_ro_Future: ì„¸ìƒì—ë‚˜â€¦                                                                                                                                                                                                                                                                                                                                                                                                                                                |\n",
      "|[ë‹¨ë…] ì •ë¶€, ì¶œìž…êµ­ ì–¼êµ´ì‚¬ì§„ 1ì–µ7ì²œë§Œê±´ AIì—…ì²´ì— ë„˜ê²¼ë‹¤ https://t.co/97weJQFyWishe wasn't the best actress of all time, but my god was she an unbelievable movie star and personalityRT @percymadraw: Un certain nombre dâ€™heures et une cinquantaine de crayons plus tardâ€¦ jâ€™ai finis ma rÃ©alisation de @Benzema ! âœï¸ðŸŽ¨                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |\n",
      "|#à¸„à¸±à¹ˆà¸™à¸à¸¹themovieLATAMRT @win_metawinbr: Please @GMMTV, we want to have the opportunity to review our dear Tine. We also need to be part of that when moment, whiâ€¦RT @chrizmillr: A storyboarding legend passed away last week. David J. Negron Sr. drew boards for the iconic Raiders boulder scene, did conâ€¦RT @t_NYC: We all understand as crew that our conditions vary and things change. We arenâ€™t asking for the entire filming schedule to revolvâ€¦Lo que mÃ¡s le agradezco a 2gether es haberme permitido conocer a los dos seres a los que mÃ¡s amo en el mundo. Graciâ€¦ https://t.co/Tj0acXtI08Not movie but a sandbox game....man i really wanted to be blown up by pekora shachou.@hacheidimarraa fala duvido ai entÃ£oRT @favelacaiunof2: O cara se casou com uma das maiores atrizes +18 do Brasil e vocÃª ai com medo de assumir a mina sÃ³ pq ela deu pra todosâ€¦Ai ai ðŸ¥±@PortalBrightWin tonhon chonlatte|\n",
      "|LATAM needs 2gether The MovieRT @__CS11: before we get the first trailer for the Uncharted movie i wanna bring back this incredible Uncharted Short Film with Nathan Filâ€¦@LegendaryEnergy That movie sounds like shit and I wouldn't want to live it-I mean watch it...RT @Thailand_Yibo: 211021 | G-SHOCK weibo update                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |\n",
      "|à¹ƒà¸™à¹‚à¸¥à¸à¹„à¸‹à¹€à¸šà¸­à¸£à¹Œà¸ˆà¸¹à¹ˆà¹‚à¸ˆà¸¡à¸­à¸¢à¹ˆà¸²à¸‡à¹„à¸¡à¹ˆà¹€à¸à¸£à¸‡à¸à¸¥à¸±à¸§ 2021 # G-SHOCK CITY BATTLE# à¸ªà¸£à¹‰à¸²à¸‡ \"à¸£à¹ˆà¸²à¸‡à¹à¸«à¹ˆà¸‡à¸­à¸™à¸²à¸„à¸•\" à¸—à¸µà¹ˆà¹€à¸«â€¦RT @nUKiOz: à¸ªà¸«à¸ à¸²à¸žà¹à¸£à¸‡à¸‡à¸²à¸™à¸£à¸§à¸¡à¸•à¸±à¸§à¸›à¸£à¸°à¸—à¹‰à¸§à¸‡à¹ƒà¸™à¹€à¸à¸²à¸«à¸¥à¸µà¹ƒà¸•à¹‰ à¹à¸•à¹ˆà¸‡à¸•à¸±à¸§à¸„à¸­à¸ªà¸•à¸¹à¸¡ #SquidGames .. à¸£à¸°à¸šà¸¸à¸§à¹ˆà¸² à¸Šà¸µà¸§à¸´à¸•à¸žà¸§à¸à¹€à¸„à¹‰à¸²à¸à¹‡à¹„à¸¡à¹ˆà¸•à¹ˆà¸²à¸‡à¸ˆà¸²à¸à¹ƒà¸™à¸‹à¸µà¸£à¸µà¸ªà¹Œà¸—à¸µà¹ˆà¸•à¹‰à¸­à¸‡à¸”à¸´à¹‰à¸™à¸£à¸™à¸«à¸²à¹€à¸¥à¸µà¹‰à¸¢à¸‡â€¦https://t.co/zH0Axvc8uh                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |\n",
      "|ä¸¸äº€è£½éººã‚„é¤ƒå­ã®çŽ‹å°†ãƒ¬ãƒ™ãƒ«ã®é£Ÿã¹ç‰©ãŒå‡ºã¦ãã‚‹è‡ªè²©æ©ŸãŒã‚ã‚‹ã¨ã„ã„ãªã€‚Ai queria tanto o tombo do Gui AraÃºjo amanhÃ£ ðŸ¥º #ProvaDoFazendeiroRT @WrittenByHanna: They have been putting Zendaya to work for a movie sheâ€™s only in for like 2 minutes ðŸ˜­ðŸ˜­RT @rude_147: https://t.co/vWMXsFEsja                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |\n",
      "|ì¸ê°„ì€ aií•œí…Œ í•„íŒ¨í• ë“¯ https://t.co/3fHcnRJrUCRT @waenaglariel: à¹€à¸ˆà¹‰à¸²à¸‚à¸­à¸‡à¹à¸­à¸žà¹€à¸§à¹‡à¸šà¸•à¸¹à¸™à¸­à¸­à¸à¹‚à¸›à¸£à¹à¸à¸£à¸¡à¸¥à¸‡à¸ªà¸µà¹à¸£à¹‰à¸§ à¸¢à¸±à¸‡à¹„à¸¡à¹ˆà¹„à¸”à¹‰à¸¥à¸­à¸‡à¹à¸•à¹ˆà¸™à¹ˆà¸²à¸ªà¸™à¹ƒà¸ˆà¸§à¹ˆà¸²à¸¡à¸±à¸™à¸ˆà¸°à¹ƒà¸Šà¹‰à¹„à¸”à¹‰à¸”à¸µà¸ˆà¸™à¸‡à¸²à¸™à¸œà¸¹à¹‰à¸Šà¹ˆà¸§à¸¢à¸¥à¸‡à¸ªà¸µà¹‚à¸”à¸™ disrupt à¸«à¸£à¸·à¸­à¹€à¸›à¸¥à¹ˆà¸² ðŸ¤”@The_MoonlightSw Ih ala conta aiai n vejo a hora de tomar minha 2Â° dose@daianenem Macho feio falando merda aiRT @10Isaraphorn: à¸„à¸¹à¹ˆà¸à¸±à¸™â€¦ à¹à¸¥à¹‰à¸§à¸¡à¸±à¸™à¸›à¸±à¸‡                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |\n",
      "|#bbrightvcRT @CosmicWonderYT: BREAKING! Marvel Studios is reportedly developing a World War Hulk movie that is going to start production on late 2022â€¦@OhMyBeatriz Amiga me passa a goblo play ai pra eu assistir tambem kkkkkkkkkai yo no qiro chisme pero el viene a mi@bonngbudi Wah pas ukuran Ai ini Ko.. Sicap Size.. kesenggol Lah Ko.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |\n",
      "|Have a great day GBU@PortalBrightWin Branco #à¸„à¸±à¹ˆà¸™à¸à¸¹themovieLATAM                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |\n",
      "|LATAM needs 2gether The MovieRT @PackFootball: Great job, # 1ï¸âƒ£ !!RT @Tropa_Do_27: SUPER BOA NOITE, SAIU O FLAY COM A LINE-UP COM TODOS OS DJ's PRA RITMAR NOSSA GRANDE NOITE,  COLA  CMG QUE E SUCESSO ðŸ”¥NOðŸŽ¡â€¦@HackedYayonne11 Moi j'ai hÃ¢te Ã  une prise de conscience collective globale que le @jdemontreal est un journal toxiâ€¦ https://t.co/z4gpsdNsxS# à¸«à¸²à¸£ ðŸŒ·Viu â™¡ à¸žà¸£à¹‰à¸­à¸¡à¸ªà¹ˆà¸‡ ðŸ’–                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame(map(lambda a: (a[0], ), tweet_array), [\"tweets\"])\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess the dataframe to have an array of words which can be sued for lda in the next stop\n",
    "# preprocessing by removing links, numbers, punctuations, hashtags etc\n",
    "\n",
    "def remove_users(tweet):\n",
    "    tweet = re.sub('(RT\\s@[A-Za-z]+[A-Za-z0-9-_]+)', '', tweet) \n",
    "    tweet = re.sub('(@[A-Za-z]+[A-Za-z0-9-_]+)', '', tweet) \n",
    "    return tweet\n",
    "\n",
    "def remove_links(tweet):\n",
    "    tweet = re.sub(r'http\\S+', '', tweet) \n",
    "    tweet = re.sub(r'bit.ly/\\S+', '', tweet) \n",
    "    tweet = tweet.strip('[link]') \n",
    "    return tweet\n",
    "\n",
    "def remove_punctuation(tweet):\n",
    "    tweet = re.sub('['+punctuation + ']+', ' ', tweet) \n",
    "    return tweet\n",
    "\n",
    "def remove_number(tweet):\n",
    "    tweet = re.sub('([0-9]+)', '', tweet) \n",
    "    return tweet\n",
    "\n",
    "def remove_hashtag(tweet):\n",
    "    tweet = re.sub('(#[A-Za-z]+[A-Za-z0-9-_]+)', '', tweet) \n",
    "    return tweet\n",
    "\n",
    "def remove_non_english(tweet):\n",
    "    tweet = re.sub(r\"[^\\x00-\\x7F]+\", '', tweet)\n",
    "    return tweet\n",
    "\n",
    "def remove_empty_words(tweet):\n",
    "    tweet = [word for word in tweet if len(word)>0]\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_links=udf(remove_links)\n",
    "remove_users=udf(remove_users)\n",
    "remove_punctuation=udf(remove_punctuation)\n",
    "remove_number=udf(remove_number)\n",
    "remove_hashtag=udf(remove_hashtag)\n",
    "remove_non_english=udf(remove_non_english)\n",
    "remove_empty_words=udf(remove_empty_words)\n",
    "df = df.withColumn('preprocessed_tweets', remove_links(df['tweets']))\n",
    "df = df.withColumn('preprocessed_tweets', remove_users(df['preprocessed_tweets']))\n",
    "df = df.withColumn('preprocessed_tweets', remove_punctuation(df['preprocessed_tweets']))\n",
    "df = df.withColumn('preprocessed_tweets', remove_number(df['preprocessed_tweets']))\n",
    "df = df.withColumn('preprocessed_tweets', remove_non_english(df['preprocessed_tweets']))\n",
    "df = df.withColumn('preprocessed_tweets', split(df['preprocessed_tweets'], ' '))\n",
    "df = df.withColumn('preprocessed_tweets', remove_empty_words(df['preprocessed_tweets']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexTokenizer().setPattern(\"[\\\\W_]+\").setMinTokenLength(3).setInputCol(\"preprocessed_tweets\").setOutputCol(\"words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_dataframe = tokenizer.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[latam, needs, gether, the, movie]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[themovielatam, winmetawin, brincando, jupy, m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[wasn, the, best, actress, all, time, but, god...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[themovielatam, please, want, have, the, oppor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[latam, needs, gether, the, moviert, before, g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[shock, city, battle, squidgames]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[queria, tanto, tombo, gui, arajo, amanh, prov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[disrupt, ala, conta, aiai, vejo, hora, tomar,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[bbrightvc, breaking, marvel, studios, reporte...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               words\n",
       "0                                                 []\n",
       "1                 [latam, needs, gether, the, movie]\n",
       "2  [themovielatam, winmetawin, brincando, jupy, m...\n",
       "3  [wasn, the, best, actress, all, time, but, god...\n",
       "4  [themovielatam, please, want, have, the, oppor...\n",
       "5  [latam, needs, gether, the, moviert, before, g...\n",
       "6                  [shock, city, battle, squidgames]\n",
       "7  [queria, tanto, tombo, gui, arajo, amanh, prov...\n",
       "8  [disrupt, ala, conta, aiai, vejo, hora, tomar,...\n",
       "9  [bbrightvc, breaking, marvel, studios, reporte..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed_dataframe = transformed_dataframe.drop(\"tweets\", \"preprocessed_tweets\")\n",
    "transformed_dataframe.printSchema()\n",
    "pd.DataFrame(transformed_dataframe.take(10), columns=transformed_dataframe.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LDA - Classification on Stream Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()\n",
    "cv.setInputCol(\"words\")\n",
    "cv.setOutputCol(\"count_vectors\")\n",
    "cv_model = cv.fit(transformed_dataframe)\n",
    "cv_model.setInputCol(\"words\")\n",
    "cv_df = cv_model.transform(transformed_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- count_vectors: vector (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>count_vectors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[]</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[latam, needs, gether, the, movie]</td>\n",
       "      <td>(1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[themovielatam, winmetawin, brincando, jupy, m...</td>\n",
       "      <td>(2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[wasn, the, best, actress, all, time, but, god...</td>\n",
       "      <td>(1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[themovielatam, please, want, have, the, oppor...</td>\n",
       "      <td>(3.0, 1.0, 2.0, 1.0, 2.0, 2.0, 2.0, 1.0, 1.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[latam, needs, gether, the, moviert, before, g...</td>\n",
       "      <td>(3.0, 2.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[shock, city, battle, squidgames]</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[queria, tanto, tombo, gui, arajo, amanh, prov...</td>\n",
       "      <td>(0.0, 1.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[disrupt, ala, conta, aiai, vejo, hora, tomar,...</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[bbrightvc, breaking, marvel, studios, reporte...</td>\n",
       "      <td>(0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               words  \\\n",
       "0                                                 []   \n",
       "1                 [latam, needs, gether, the, movie]   \n",
       "2  [themovielatam, winmetawin, brincando, jupy, m...   \n",
       "3  [wasn, the, best, actress, all, time, but, god...   \n",
       "4  [themovielatam, please, want, have, the, oppor...   \n",
       "5  [latam, needs, gether, the, moviert, before, g...   \n",
       "6                  [shock, city, battle, squidgames]   \n",
       "7  [queria, tanto, tombo, gui, arajo, amanh, prov...   \n",
       "8  [disrupt, ala, conta, aiai, vejo, hora, tomar,...   \n",
       "9  [bbrightvc, breaking, marvel, studios, reporte...   \n",
       "\n",
       "                                       count_vectors  \n",
       "0  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "1  (1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "2  (2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, ...  \n",
       "3  (1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, ...  \n",
       "4  (3.0, 1.0, 2.0, 1.0, 2.0, 2.0, 2.0, 1.0, 1.0, ...  \n",
       "5  (3.0, 2.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, ...  \n",
       "6  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "7  (0.0, 1.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "8  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "9  (0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, ...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_df.printSchema()\n",
    "pd.DataFrame(cv_df.take(10), columns=cv_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LDA(featuresCol=\"count_vectors\", k=10)\n",
    "lda_model = lda.fit(cv_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- topic: integer (nullable = false)\n",
      " |-- termIndices: array (nullable = true)\n",
      " |    |-- element: integer (containsNull = false)\n",
      " |-- termWeights: array (nullable = true)\n",
      " |    |-- element: double (containsNull = false)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic</th>\n",
       "      <th>termIndices</th>\n",
       "      <th>termWeights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[11, 2, 170, 1, 177, 113, 57, 216, 166, 138]</td>\n",
       "      <td>[0.007491742205836237, 0.006514935777410096, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[179, 47, 163, 77, 238, 161, 200, 141, 245, 97]</td>\n",
       "      <td>[0.005113887875789274, 0.0050726772149615, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[167, 119, 230, 48, 1, 126, 172, 161, 219, 39]</td>\n",
       "      <td>[0.005199508682557248, 0.005172873605263552, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[17, 5, 4, 1, 0, 9, 10, 3, 13, 140]</td>\n",
       "      <td>[0.011594834103265384, 0.011533603747157443, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[92, 240, 135, 126, 239, 85, 128, 56, 45, 205]</td>\n",
       "      <td>[0.00883759827781119, 0.008472574434990018, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>[8, 139, 237, 185, 180, 113, 163, 199, 219, 97]</td>\n",
       "      <td>[0.005176307993981117, 0.005082828233701475, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>[1, 0, 9, 10, 60, 36, 240, 222, 206, 59]</td>\n",
       "      <td>[0.006073734114041892, 0.006062915008160177, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>[175, 7, 183, 192, 23, 40, 152, 16, 49, 43]</td>\n",
       "      <td>[0.007505569913712284, 0.007278665927345907, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>[214, 98, 222, 85, 89, 13, 62, 56, 5, 74]</td>\n",
       "      <td>[0.004990908947394858, 0.004894285541201307, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>[0, 146, 91, 182, 7, 12, 159, 229, 233, 42]</td>\n",
       "      <td>[0.006518755937754465, 0.006247457472250252, 0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   topic                                      termIndices  \\\n",
       "0      0     [11, 2, 170, 1, 177, 113, 57, 216, 166, 138]   \n",
       "1      1  [179, 47, 163, 77, 238, 161, 200, 141, 245, 97]   \n",
       "2      2   [167, 119, 230, 48, 1, 126, 172, 161, 219, 39]   \n",
       "3      3              [17, 5, 4, 1, 0, 9, 10, 3, 13, 140]   \n",
       "4      4   [92, 240, 135, 126, 239, 85, 128, 56, 45, 205]   \n",
       "5      5  [8, 139, 237, 185, 180, 113, 163, 199, 219, 97]   \n",
       "6      6         [1, 0, 9, 10, 60, 36, 240, 222, 206, 59]   \n",
       "7      7      [175, 7, 183, 192, 23, 40, 152, 16, 49, 43]   \n",
       "8      8        [214, 98, 222, 85, 89, 13, 62, 56, 5, 74]   \n",
       "9      9      [0, 146, 91, 182, 7, 12, 159, 229, 233, 42]   \n",
       "\n",
       "                                         termWeights  \n",
       "0  [0.007491742205836237, 0.006514935777410096, 0...  \n",
       "1  [0.005113887875789274, 0.0050726772149615, 0.0...  \n",
       "2  [0.005199508682557248, 0.005172873605263552, 0...  \n",
       "3  [0.011594834103265384, 0.011533603747157443, 0...  \n",
       "4  [0.00883759827781119, 0.008472574434990018, 0....  \n",
       "5  [0.005176307993981117, 0.005082828233701475, 0...  \n",
       "6  [0.006073734114041892, 0.006062915008160177, 0...  \n",
       "7  [0.007505569913712284, 0.007278665927345907, 0...  \n",
       "8  [0.004990908947394858, 0.004894285541201307, 0...  \n",
       "9  [0.006518755937754465, 0.006247457472250252, 0...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics = lda_model.describeTopics(10)\n",
    "topics.printSchema()\n",
    "pd.DataFrame(topics.take(10), columns=topics.columns) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topic Rendering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|topicDistribution                                                                                                                                                                                                      |\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]                                                                                                                                                                              |\n",
      "|[0.016153501631012014,0.015096157605701883,0.015096242810481601,0.8605464226632783,0.01609209799452398,0.015096228037751205,0.01533350617615368,0.015844153545235645,0.015096162175575809,0.0156455273602857]          |\n",
      "|[0.004176886406046546,0.003903465310110971,0.003903475447700312,0.004441436133462026,0.004160949813463183,0.0039034673891025955,0.00396476160196634,0.004096874223483171,0.003903465995451882,0.963545217679213]       |\n",
      "|[0.003556373640748802,0.003323587035454282,0.0033235888691092925,0.0037814783776096393,0.0035428046938924527,0.0033235899211552056,0.0033757649783879694,0.9690046701960151,0.0033235835702986414,0.003444558717328764]|\n",
      "|[9.584811043046256E-4,8.95684270720945E-4,8.956862614201325E-4,0.30693985150001774,9.548057364136112E-4,8.956850988248731E-4,9.097563932162618E-4,9.400892581218821E-4,0.6856816545464539,9.283058305059022E-4]        |\n",
      "|[0.0023987525022572515,0.0022416058381048974,0.0022416200741487186,0.002550943968567682,0.002389578316965692,0.0022416132001711955,0.9790183161547638,0.002352733128424961,0.0022416005603944503,0.0023232362562014565]|\n",
      "|[0.01943395859608845,0.01816057964124918,0.01816069457732294,0.02066433772372389,0.019358793147843924,0.018160642904791597,0.018446304982294888,0.01906097347745136,0.8297319243598023,0.018821790589431453]           |\n",
      "|[0.9602055363233747,0.004276484864552441,0.004276505630020631,0.0048657480837017725,0.004558624785054311,0.0042764989131612715,0.004343640832857735,0.004488331722947096,0.0042764795896053824,0.004432149254724646]   |\n",
      "|[0.006012942768527408,0.005619104790957646,0.9473174182047533,0.006393689075486587,0.005989856447650572,0.005619109577375843,0.0057073914336456975,0.0058976214887529205,0.005619151816585288,0.005823714396264861]    |\n",
      "|[0.00266547092971718,0.002491013548081264,0.002491018176822995,0.0028340354381593954,0.9768103013747741,0.0024910143928997025,0.0025301128768463947,0.0026143680681207353,0.002491017320673392,0.0025816478739048676]  |\n",
      "|[0.8797947317725766,0.012917656632073723,0.012917731136343898,0.014698566866248963,0.013769776925757168,0.012917599633975143,0.013120582983086146,0.013557604224538488,0.012917717278862512,0.013388032546537414]      |\n",
      "|[0.002593275198115026,0.002423552410541238,0.002423558544141977,0.9776121844950278,0.0025833972030038384,0.00242355614042982,0.0024616014273331926,0.002543576481251578,0.0024235525121072176,0.0025117455880482712]   |\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transformed = lda_model.transform(cv_df).select(\"topicDistribution\")  \n",
    "transformed.show(truncate=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ll:  -4160.19803443369\n",
      "lp:  13.551133662650455\n"
     ]
    }
   ],
   "source": [
    "ll = lda_model.logLikelihood(cv_df)  \n",
    "lp = lda_model.logPerplexity(cv_df)\n",
    "print(\"ll: \", ll)\n",
    "print(\"lp: \", lp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned topics (as distributions over vocab of 250 words):\n",
      "DenseMatrix([[0.67021367, 0.81626506, 0.77396337, ..., 1.36951606, 0.84415586,\n",
      "              1.2977404 ],\n",
      "             [1.17242519, 0.62382319, 0.93440961, ..., 1.30128548, 0.73366457,\n",
      "              0.64164861],\n",
      "             [1.31198274, 0.8150819 , 0.83428161, ..., 0.80464391, 0.64943342,\n",
      "              0.83267468],\n",
      "             ...,\n",
      "             [0.85542488, 0.76398576, 0.83763683, ..., 0.79912621, 0.74905216,\n",
      "              0.8248356 ],\n",
      "             [1.08716536, 0.75273105, 0.79605102, ..., 0.68827195, 0.77213337,\n",
      "              0.72626644],\n",
      "             [0.80212443, 0.65958154, 0.79308804, ..., 0.6447466 , 0.71329115,\n",
      "              0.69264659]])\n"
     ]
    }
   ],
   "source": [
    "print(\"Learned topics (as distributions over vocab of 250 words):\")\n",
    "topics_matrix = lda_model.topicsMatrix()\n",
    "print(topics_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
