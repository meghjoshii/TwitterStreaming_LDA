{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pyspark import SparkConf, SparkContext, SQLContext \n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import split, explode, udf, lit, size, col\n",
    "from pyspark.ml.feature import RegexTokenizer, CountVectorizer\n",
    "from pyspark.ml.clustering import LDA\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"DataFrame\").getOrCreate()\n",
    "sc = SparkContext.getOrCreate()\n",
    "rdd = sc.wholeTextFiles(\"gs://mmj2169hw2/hadoop/tmp/bigquery/pyspark_output/lda_file/lda.txt/*\")\n",
    "\n",
    "tweets = rdd.map(lambda x: x[1].split('\\n'))\n",
    "tweet_array = tweets.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- tweets: string (nullable = true)\n",
      "\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|tweets                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|‡∏£‡∏±‡∏ö‡∏á‡∏≤‡∏ô ‡∏°‡∏≤‡πÉ‡∏´‡∏°‡πà‡∏Ñ‡πà‡∏∞ ‡∏ß‡πà‡∏ß‡∏á‡∏Ñ‡πà‡∏∞                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |\n",
      "|# ‡∏£‡∏±‚Ä¶ https://t.co/4ujyL8e2vXRT @luvswbn: LATAM needs 2gether the movie                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |\n",
      "|#‡∏Ñ‡∏±‡πà‡∏ô‡∏Å‡∏πthemovieLATAM #winmetawin @winmetawin https://t.co/rkwASdDmDb@scodarko to brincando Jupy mais e ai vc ta bem depois de tudo isso?ÿπÿßÿµŸÖ€Å ÿ¥€åÿ±ÿßÿ≤€å ÿ¨€åÿ≥€å ÿ≠ÿ±ÿßŸÖ ÿÆŸàÿ± ÿπŸàÿ±ÿ™Ÿà⁄∫ ÿßŸàÿ± ŸÖÿ±ÿØŸà⁄∫ ŸÖ€å⁄∫ ÿ≠ÿ±ÿßŸÖ ⁄©€í Ÿæ€åÿ≥€í  ÿßŸÜ⁄©€å ÿ±Ÿàÿ≠ ⁄©€å ÿ∫ÿ∞ÿß ÿ®ŸÜ ⁄Ü⁄©€í €Å€å⁄∫€î€å€Å ÿ¨ÿ® ÿ™⁄© ÿ≠ÿ±ÿßŸÖ ⁄©ÿß ŸÑŸÇŸÖ€Å ⁄©⁄æÿß‚Ä¶ https://t.co/rjOi9DUD6lRT @yenaviral: \"The 71st episode of ÎΩïÏà≠ÏïÑÌïôÎãπ, aired on the 20th, soared to 5.9% nationwide and 7.1% per minute according to Nielsen Korea, ran‚Ä¶RT @Hell_ro_Future: ÏÑ∏ÏÉÅÏóêÎÇò‚Ä¶                                                                                                                                                                                                                                                                                                                                                                                                                                                |\n",
      "|[Îã®ÎèÖ] Ï†ïÎ∂Ä, Ï∂úÏûÖÍµ≠ ÏñºÍµ¥ÏÇ¨ÏßÑ 1Ïñµ7Ï≤úÎßåÍ±¥ AIÏóÖÏ≤¥Ïóê ÎÑòÍ≤ºÎã§ https://t.co/97weJQFyWishe wasn't the best actress of all time, but my god was she an unbelievable movie star and personalityRT @percymadraw: Un certain nombre d‚Äôheures et une cinquantaine de crayons plus tard‚Ä¶ j‚Äôai finis ma r√©alisation de @Benzema ! ‚úçÔ∏èüé®                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |\n",
      "|#‡∏Ñ‡∏±‡πà‡∏ô‡∏Å‡∏πthemovieLATAMRT @win_metawinbr: Please @GMMTV, we want to have the opportunity to review our dear Tine. We also need to be part of that when moment, whi‚Ä¶RT @chrizmillr: A storyboarding legend passed away last week. David J. Negron Sr. drew boards for the iconic Raiders boulder scene, did con‚Ä¶RT @t_NYC: We all understand as crew that our conditions vary and things change. We aren‚Äôt asking for the entire filming schedule to revolv‚Ä¶Lo que m√°s le agradezco a 2gether es haberme permitido conocer a los dos seres a los que m√°s amo en el mundo. Graci‚Ä¶ https://t.co/Tj0acXtI08Not movie but a sandbox game....man i really wanted to be blown up by pekora shachou.@hacheidimarraa fala duvido ai ent√£oRT @favelacaiunof2: O cara se casou com uma das maiores atrizes +18 do Brasil e voc√™ ai com medo de assumir a mina s√≥ pq ela deu pra todos‚Ä¶Ai ai ü•±@PortalBrightWin tonhon chonlatte|\n",
      "|LATAM needs 2gether The MovieRT @__CS11: before we get the first trailer for the Uncharted movie i wanna bring back this incredible Uncharted Short Film with Nathan Fil‚Ä¶@LegendaryEnergy That movie sounds like shit and I wouldn't want to live it-I mean watch it...RT @Thailand_Yibo: 211021 | G-SHOCK weibo update                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |\n",
      "|‡πÉ‡∏ô‡πÇ‡∏•‡∏Å‡πÑ‡∏ã‡πÄ‡∏ö‡∏≠‡∏£‡πå‡∏à‡∏π‡πà‡πÇ‡∏à‡∏°‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏£‡∏á‡∏Å‡∏•‡∏±‡∏ß 2021 # G-SHOCK CITY BATTLE# ‡∏™‡∏£‡πâ‡∏≤‡∏á \"‡∏£‡πà‡∏≤‡∏á‡πÅ‡∏´‡πà‡∏á‡∏≠‡∏ô‡∏≤‡∏Ñ‡∏ï\" ‡∏ó‡∏µ‡πà‡πÄ‡∏´‚Ä¶RT @nUKiOz: ‡∏™‡∏´‡∏†‡∏≤‡∏û‡πÅ‡∏£‡∏á‡∏á‡∏≤‡∏ô‡∏£‡∏ß‡∏°‡∏ï‡∏±‡∏ß‡∏õ‡∏£‡∏∞‡∏ó‡πâ‡∏ß‡∏á‡πÉ‡∏ô‡πÄ‡∏Å‡∏≤‡∏´‡∏•‡∏µ‡πÉ‡∏ï‡πâ ‡πÅ‡∏ï‡πà‡∏á‡∏ï‡∏±‡∏ß‡∏Ñ‡∏≠‡∏™‡∏ï‡∏π‡∏° #SquidGames .. ‡∏£‡∏∞‡∏ö‡∏∏‡∏ß‡πà‡∏≤ ‡∏ä‡∏µ‡∏ß‡∏¥‡∏ï‡∏û‡∏ß‡∏Å‡πÄ‡∏Ñ‡πâ‡∏≤‡∏Å‡πá‡πÑ‡∏°‡πà‡∏ï‡πà‡∏≤‡∏á‡∏à‡∏≤‡∏Å‡πÉ‡∏ô‡∏ã‡∏µ‡∏£‡∏µ‡∏™‡πå‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏î‡∏¥‡πâ‡∏ô‡∏£‡∏ô‡∏´‡∏≤‡πÄ‡∏•‡∏µ‡πâ‡∏¢‡∏á‚Ä¶https://t.co/zH0Axvc8uh                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |\n",
      "|‰∏∏‰∫ÄË£ΩÈ∫∫„ÇÑÈ§ÉÂ≠ê„ÅÆÁéãÂ∞Ü„É¨„Éô„É´„ÅÆÈ£ü„ÅπÁâ©„ÅåÂá∫„Å¶„Åè„ÇãËá™Ë≤©Ê©ü„Åå„ÅÇ„Çã„Å®„ÅÑ„ÅÑ„Å™„ÄÇAi queria tanto o tombo do Gui Ara√∫jo amanh√£ ü•∫ #ProvaDoFazendeiroRT @WrittenByHanna: They have been putting Zendaya to work for a movie she‚Äôs only in for like 2 minutes üò≠üò≠RT @rude_147: https://t.co/vWMXsFEsja                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |\n",
      "|Ïù∏Í∞ÑÏùÄ aiÌïúÌÖå ÌïÑÌå®Ìï†ÎìØ https://t.co/3fHcnRJrUCRT @waenaglariel: ‡πÄ‡∏à‡πâ‡∏≤‡∏Ç‡∏≠‡∏á‡πÅ‡∏≠‡∏û‡πÄ‡∏ß‡πá‡∏ö‡∏ï‡∏π‡∏ô‡∏≠‡∏≠‡∏Å‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏°‡∏•‡∏á‡∏™‡∏µ‡πÅ‡∏£‡πâ‡∏ß ‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏•‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏ô‡πà‡∏≤‡∏™‡∏ô‡πÉ‡∏à‡∏ß‡πà‡∏≤‡∏°‡∏±‡∏ô‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ‡∏î‡∏µ‡∏à‡∏ô‡∏á‡∏≤‡∏ô‡∏ú‡∏π‡πâ‡∏ä‡πà‡∏ß‡∏¢‡∏•‡∏á‡∏™‡∏µ‡πÇ‡∏î‡∏ô disrupt ‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏õ‡∏•‡πà‡∏≤ ü§î@The_MoonlightSw Ih ala conta aiai n vejo a hora de tomar minha 2¬∞ dose@daianenem Macho feio falando merda aiRT @10Isaraphorn: ‡∏Ñ‡∏π‡πà‡∏Å‡∏±‡∏ô‚Ä¶ ‡πÅ‡∏•‡πâ‡∏ß‡∏°‡∏±‡∏ô‡∏õ‡∏±‡∏á                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |\n",
      "|#bbrightvcRT @CosmicWonderYT: BREAKING! Marvel Studios is reportedly developing a World War Hulk movie that is going to start production on late 2022‚Ä¶@OhMyBeatriz Amiga me passa a goblo play ai pra eu assistir tambem kkkkkkkkkai yo no qiro chisme pero el viene a mi@bonngbudi Wah pas ukuran Ai ini Ko.. Sicap Size.. kesenggol Lah Ko.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |\n",
      "|Have a great day GBU@PortalBrightWin Branco #‡∏Ñ‡∏±‡πà‡∏ô‡∏Å‡∏πthemovieLATAM                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |\n",
      "|LATAM needs 2gether The MovieRT @PackFootball: Great job, # 1Ô∏è‚É£ !!RT @Tropa_Do_27: SUPER BOA NOITE, SAIU O FLAY COM A LINE-UP COM TODOS OS DJ's PRA RITMAR NOSSA GRANDE NOITE,  COLA  CMG QUE E SUCESSO üî•NOüé°‚Ä¶@HackedYayonne11 Moi j'ai h√¢te √† une prise de conscience collective globale que le @jdemontreal est un journal toxi‚Ä¶ https://t.co/z4gpsdNsxS# ‡∏´‡∏≤‡∏£ üå∑Viu ‚ô° ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏™‡πà‡∏á üíñ                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame(map(lambda a: (a[0], ), tweet_array), [\"tweets\"])\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess the dataframe to have an array of words which can be sued for lda in the next stop\n",
    "# preprocessing by removing links, numbers, punctuations, hashtags etc\n",
    "\n",
    "def remove_users(tweet):\n",
    "    tweet = re.sub('(RT\\s@[A-Za-z]+[A-Za-z0-9-_]+)', '', tweet) \n",
    "    tweet = re.sub('(@[A-Za-z]+[A-Za-z0-9-_]+)', '', tweet) \n",
    "    return tweet\n",
    "\n",
    "def remove_links(tweet):\n",
    "    tweet = re.sub(r'http\\S+', '', tweet) \n",
    "    tweet = re.sub(r'bit.ly/\\S+', '', tweet) \n",
    "    tweet = tweet.strip('[link]') \n",
    "    return tweet\n",
    "\n",
    "def remove_punctuation(tweet):\n",
    "    tweet = re.sub('['+punctuation + ']+', ' ', tweet) \n",
    "    return tweet\n",
    "\n",
    "def remove_number(tweet):\n",
    "    tweet = re.sub('([0-9]+)', '', tweet) \n",
    "    return tweet\n",
    "\n",
    "def remove_hashtag(tweet):\n",
    "    tweet = re.sub('(#[A-Za-z]+[A-Za-z0-9-_]+)', '', tweet) \n",
    "    return tweet\n",
    "\n",
    "def remove_non_english(tweet):\n",
    "    tweet = re.sub(r\"[^\\x00-\\x7F]+\", '', tweet)\n",
    "    return tweet\n",
    "\n",
    "def remove_empty_words(tweet):\n",
    "    tweet = [word for word in tweet if len(word)>0]\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_links=udf(remove_links)\n",
    "remove_users=udf(remove_users)\n",
    "remove_punctuation=udf(remove_punctuation)\n",
    "remove_number=udf(remove_number)\n",
    "remove_hashtag=udf(remove_hashtag)\n",
    "remove_non_english=udf(remove_non_english)\n",
    "remove_empty_words=udf(remove_empty_words)\n",
    "df = df.withColumn('preprocessed_tweets', remove_links(df['tweets']))\n",
    "df = df.withColumn('preprocessed_tweets', remove_users(df['preprocessed_tweets']))\n",
    "df = df.withColumn('preprocessed_tweets', remove_punctuation(df['preprocessed_tweets']))\n",
    "df = df.withColumn('preprocessed_tweets', remove_number(df['preprocessed_tweets']))\n",
    "df = df.withColumn('preprocessed_tweets', remove_non_english(df['preprocessed_tweets']))\n",
    "df = df.withColumn('preprocessed_tweets', split(df['preprocessed_tweets'], ' '))\n",
    "df = df.withColumn('preprocessed_tweets', remove_empty_words(df['preprocessed_tweets']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexTokenizer().setPattern(\"[\\\\W_]+\").setMinTokenLength(3).setInputCol(\"preprocessed_tweets\").setOutputCol(\"words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_dataframe = tokenizer.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[latam, needs, gether, the, movie]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[themovielatam, winmetawin, brincando, jupy, m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[wasn, the, best, actress, all, time, but, god...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[themovielatam, please, want, have, the, oppor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[latam, needs, gether, the, moviert, before, g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[shock, city, battle, squidgames]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[queria, tanto, tombo, gui, arajo, amanh, prov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[disrupt, ala, conta, aiai, vejo, hora, tomar,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[bbrightvc, breaking, marvel, studios, reporte...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               words\n",
       "0                                                 []\n",
       "1                 [latam, needs, gether, the, movie]\n",
       "2  [themovielatam, winmetawin, brincando, jupy, m...\n",
       "3  [wasn, the, best, actress, all, time, but, god...\n",
       "4  [themovielatam, please, want, have, the, oppor...\n",
       "5  [latam, needs, gether, the, moviert, before, g...\n",
       "6                  [shock, city, battle, squidgames]\n",
       "7  [queria, tanto, tombo, gui, arajo, amanh, prov...\n",
       "8  [disrupt, ala, conta, aiai, vejo, hora, tomar,...\n",
       "9  [bbrightvc, breaking, marvel, studios, reporte..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed_dataframe = transformed_dataframe.drop(\"tweets\", \"preprocessed_tweets\")\n",
    "transformed_dataframe.printSchema()\n",
    "pd.DataFrame(transformed_dataframe.take(10), columns=transformed_dataframe.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LDA - Classification on Stream Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()\n",
    "cv.setInputCol(\"words\")\n",
    "cv.setOutputCol(\"count_vectors\")\n",
    "cv_model = cv.fit(transformed_dataframe)\n",
    "cv_model.setInputCol(\"words\")\n",
    "cv_df = cv_model.transform(transformed_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- count_vectors: vector (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>count_vectors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[]</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[latam, needs, gether, the, movie]</td>\n",
       "      <td>(1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[themovielatam, winmetawin, brincando, jupy, m...</td>\n",
       "      <td>(2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[wasn, the, best, actress, all, time, but, god...</td>\n",
       "      <td>(1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[themovielatam, please, want, have, the, oppor...</td>\n",
       "      <td>(3.0, 1.0, 2.0, 1.0, 2.0, 2.0, 2.0, 1.0, 1.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[latam, needs, gether, the, moviert, before, g...</td>\n",
       "      <td>(3.0, 2.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[shock, city, battle, squidgames]</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[queria, tanto, tombo, gui, arajo, amanh, prov...</td>\n",
       "      <td>(0.0, 1.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[disrupt, ala, conta, aiai, vejo, hora, tomar,...</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[bbrightvc, breaking, marvel, studios, reporte...</td>\n",
       "      <td>(0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               words  \\\n",
       "0                                                 []   \n",
       "1                 [latam, needs, gether, the, movie]   \n",
       "2  [themovielatam, winmetawin, brincando, jupy, m...   \n",
       "3  [wasn, the, best, actress, all, time, but, god...   \n",
       "4  [themovielatam, please, want, have, the, oppor...   \n",
       "5  [latam, needs, gether, the, moviert, before, g...   \n",
       "6                  [shock, city, battle, squidgames]   \n",
       "7  [queria, tanto, tombo, gui, arajo, amanh, prov...   \n",
       "8  [disrupt, ala, conta, aiai, vejo, hora, tomar,...   \n",
       "9  [bbrightvc, breaking, marvel, studios, reporte...   \n",
       "\n",
       "                                       count_vectors  \n",
       "0  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "1  (1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "2  (2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, ...  \n",
       "3  (1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, ...  \n",
       "4  (3.0, 1.0, 2.0, 1.0, 2.0, 2.0, 2.0, 1.0, 1.0, ...  \n",
       "5  (3.0, 2.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, ...  \n",
       "6  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "7  (0.0, 1.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "8  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "9  (0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, ...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_df.printSchema()\n",
    "pd.DataFrame(cv_df.take(10), columns=cv_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LDA(featuresCol=\"count_vectors\", k=10)\n",
    "lda_model = lda.fit(cv_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- topic: integer (nullable = false)\n",
      " |-- termIndices: array (nullable = true)\n",
      " |    |-- element: integer (containsNull = false)\n",
      " |-- termWeights: array (nullable = true)\n",
      " |    |-- element: double (containsNull = false)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic</th>\n",
       "      <th>termIndices</th>\n",
       "      <th>termWeights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[11, 2, 170, 1, 177, 113, 57, 216, 166, 138]</td>\n",
       "      <td>[0.007491742205836237, 0.006514935777410096, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[179, 47, 163, 77, 238, 161, 200, 141, 245, 97]</td>\n",
       "      <td>[0.005113887875789274, 0.0050726772149615, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[167, 119, 230, 48, 1, 126, 172, 161, 219, 39]</td>\n",
       "      <td>[0.005199508682557248, 0.005172873605263552, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[17, 5, 4, 1, 0, 9, 10, 3, 13, 140]</td>\n",
       "      <td>[0.011594834103265384, 0.011533603747157443, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[92, 240, 135, 126, 239, 85, 128, 56, 45, 205]</td>\n",
       "      <td>[0.00883759827781119, 0.008472574434990018, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>[8, 139, 237, 185, 180, 113, 163, 199, 219, 97]</td>\n",
       "      <td>[0.005176307993981117, 0.005082828233701475, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>[1, 0, 9, 10, 60, 36, 240, 222, 206, 59]</td>\n",
       "      <td>[0.006073734114041892, 0.006062915008160177, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>[175, 7, 183, 192, 23, 40, 152, 16, 49, 43]</td>\n",
       "      <td>[0.007505569913712284, 0.007278665927345907, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>[214, 98, 222, 85, 89, 13, 62, 56, 5, 74]</td>\n",
       "      <td>[0.004990908947394858, 0.004894285541201307, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>[0, 146, 91, 182, 7, 12, 159, 229, 233, 42]</td>\n",
       "      <td>[0.006518755937754465, 0.006247457472250252, 0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   topic                                      termIndices  \\\n",
       "0      0     [11, 2, 170, 1, 177, 113, 57, 216, 166, 138]   \n",
       "1      1  [179, 47, 163, 77, 238, 161, 200, 141, 245, 97]   \n",
       "2      2   [167, 119, 230, 48, 1, 126, 172, 161, 219, 39]   \n",
       "3      3              [17, 5, 4, 1, 0, 9, 10, 3, 13, 140]   \n",
       "4      4   [92, 240, 135, 126, 239, 85, 128, 56, 45, 205]   \n",
       "5      5  [8, 139, 237, 185, 180, 113, 163, 199, 219, 97]   \n",
       "6      6         [1, 0, 9, 10, 60, 36, 240, 222, 206, 59]   \n",
       "7      7      [175, 7, 183, 192, 23, 40, 152, 16, 49, 43]   \n",
       "8      8        [214, 98, 222, 85, 89, 13, 62, 56, 5, 74]   \n",
       "9      9      [0, 146, 91, 182, 7, 12, 159, 229, 233, 42]   \n",
       "\n",
       "                                         termWeights  \n",
       "0  [0.007491742205836237, 0.006514935777410096, 0...  \n",
       "1  [0.005113887875789274, 0.0050726772149615, 0.0...  \n",
       "2  [0.005199508682557248, 0.005172873605263552, 0...  \n",
       "3  [0.011594834103265384, 0.011533603747157443, 0...  \n",
       "4  [0.00883759827781119, 0.008472574434990018, 0....  \n",
       "5  [0.005176307993981117, 0.005082828233701475, 0...  \n",
       "6  [0.006073734114041892, 0.006062915008160177, 0...  \n",
       "7  [0.007505569913712284, 0.007278665927345907, 0...  \n",
       "8  [0.004990908947394858, 0.004894285541201307, 0...  \n",
       "9  [0.006518755937754465, 0.006247457472250252, 0...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics = lda_model.describeTopics(10)\n",
    "topics.printSchema()\n",
    "pd.DataFrame(topics.take(10), columns=topics.columns) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topic Rendering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|topicDistribution                                                                                                                                                                                                      |\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]                                                                                                                                                                              |\n",
      "|[0.016153501631012014,0.015096157605701883,0.015096242810481601,0.8605464226632783,0.01609209799452398,0.015096228037751205,0.01533350617615368,0.015844153545235645,0.015096162175575809,0.0156455273602857]          |\n",
      "|[0.004176886406046546,0.003903465310110971,0.003903475447700312,0.004441436133462026,0.004160949813463183,0.0039034673891025955,0.00396476160196634,0.004096874223483171,0.003903465995451882,0.963545217679213]       |\n",
      "|[0.003556373640748802,0.003323587035454282,0.0033235888691092925,0.0037814783776096393,0.0035428046938924527,0.0033235899211552056,0.0033757649783879694,0.9690046701960151,0.0033235835702986414,0.003444558717328764]|\n",
      "|[9.584811043046256E-4,8.95684270720945E-4,8.956862614201325E-4,0.30693985150001774,9.548057364136112E-4,8.956850988248731E-4,9.097563932162618E-4,9.400892581218821E-4,0.6856816545464539,9.283058305059022E-4]        |\n",
      "|[0.0023987525022572515,0.0022416058381048974,0.0022416200741487186,0.002550943968567682,0.002389578316965692,0.0022416132001711955,0.9790183161547638,0.002352733128424961,0.0022416005603944503,0.0023232362562014565]|\n",
      "|[0.01943395859608845,0.01816057964124918,0.01816069457732294,0.02066433772372389,0.019358793147843924,0.018160642904791597,0.018446304982294888,0.01906097347745136,0.8297319243598023,0.018821790589431453]           |\n",
      "|[0.9602055363233747,0.004276484864552441,0.004276505630020631,0.0048657480837017725,0.004558624785054311,0.0042764989131612715,0.004343640832857735,0.004488331722947096,0.0042764795896053824,0.004432149254724646]   |\n",
      "|[0.006012942768527408,0.005619104790957646,0.9473174182047533,0.006393689075486587,0.005989856447650572,0.005619109577375843,0.0057073914336456975,0.0058976214887529205,0.005619151816585288,0.005823714396264861]    |\n",
      "|[0.00266547092971718,0.002491013548081264,0.002491018176822995,0.0028340354381593954,0.9768103013747741,0.0024910143928997025,0.0025301128768463947,0.0026143680681207353,0.002491017320673392,0.0025816478739048676]  |\n",
      "|[0.8797947317725766,0.012917656632073723,0.012917731136343898,0.014698566866248963,0.013769776925757168,0.012917599633975143,0.013120582983086146,0.013557604224538488,0.012917717278862512,0.013388032546537414]      |\n",
      "|[0.002593275198115026,0.002423552410541238,0.002423558544141977,0.9776121844950278,0.0025833972030038384,0.00242355614042982,0.0024616014273331926,0.002543576481251578,0.0024235525121072176,0.0025117455880482712]   |\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transformed = lda_model.transform(cv_df).select(\"topicDistribution\")  \n",
    "transformed.show(truncate=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ll:  -4160.19803443369\n",
      "lp:  13.551133662650455\n"
     ]
    }
   ],
   "source": [
    "ll = lda_model.logLikelihood(cv_df)  \n",
    "lp = lda_model.logPerplexity(cv_df)\n",
    "print(\"ll: \", ll)\n",
    "print(\"lp: \", lp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned topics (as distributions over vocab of 250 words):\n",
      "DenseMatrix([[0.67021367, 0.81626506, 0.77396337, ..., 1.36951606, 0.84415586,\n",
      "              1.2977404 ],\n",
      "             [1.17242519, 0.62382319, 0.93440961, ..., 1.30128548, 0.73366457,\n",
      "              0.64164861],\n",
      "             [1.31198274, 0.8150819 , 0.83428161, ..., 0.80464391, 0.64943342,\n",
      "              0.83267468],\n",
      "             ...,\n",
      "             [0.85542488, 0.76398576, 0.83763683, ..., 0.79912621, 0.74905216,\n",
      "              0.8248356 ],\n",
      "             [1.08716536, 0.75273105, 0.79605102, ..., 0.68827195, 0.77213337,\n",
      "              0.72626644],\n",
      "             [0.80212443, 0.65958154, 0.79308804, ..., 0.6447466 , 0.71329115,\n",
      "              0.69264659]])\n"
     ]
    }
   ],
   "source": [
    "print(\"Learned topics (as distributions over vocab of 250 words):\")\n",
    "topics_matrix = lda_model.topicsMatrix()\n",
    "print(topics_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
